{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MtTake/PINNs/blob/main/2D_Navier_Eq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mObgzhvZTSWU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nKlEzCEXe6M"
      },
      "source": [
        "# 弾性力学の支配方程式\n",
        "\n",
        "- つり合い式 \\\\\n",
        "- 適合条件式(変位-ひずみ関係式)\n",
        "- 構成式(応力-ひずみ関係式) \\\\\n",
        "\n",
        "### 1. つり合い式\n",
        "　二次元で考えると、\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\sigma_x}{\\partial x} + \\frac{\\partial \\tau_{xy}}{\\partial y} + X = 0 \\\\\n",
        "\\frac{\\partial \\sigma_y}{\\partial y} + \\frac{\\partial \\tau_{xy}}{\\partial x} + Y = 0 \\\\\n",
        " $$\n",
        "\n",
        "### 2. 適合条件式\n",
        "\n",
        "$$\n",
        "\\frac{\\partial^2 \\epsilon_x}{\\partial y^2} + \\frac{\\partial^2 \\epsilon_y}{\\partial x^2} = \\frac{\\partial^2 \\gamma_{xy}}{\\partial x \\partial y} \\\\\n",
        "\\epsilon_x = \\frac{\\partial u}{\\partial x}, \\quad\n",
        "\\epsilon_y = \\frac{\\partial v}{\\partial y}, \\quad\n",
        "\\gamma_{xy} = \\frac{\\partial u}{\\partial y} + \\frac{\\partial v}{\\partial x}\n",
        "$$\n",
        "\n",
        "### 3. 構成式\n",
        "\n",
        "$$\n",
        "\\begin {Bmatrix}\n",
        "       \\epsilon_x \\\\\n",
        "       \\epsilon_y \\\\\n",
        "       \\epsilon_z \\\\\n",
        "       \\gamma_{xy} \\\\\n",
        "       \\gamma_{yz} \\\\\n",
        "       \\gamma_{zx}\n",
        "\\end {Bmatrix}\n",
        "= \\frac{1}{E}\n",
        "\\begin {bmatrix}\n",
        "       1 & -v & -v & 0 & 0 & 0 \\\\\n",
        "       -v & 1 & -v & 0 & 0 & 0 \\\\\n",
        "       -v & -v & 1 & 0 & 0 & 0 \\\\\n",
        "       0 & 0 & 0 & 2(1+v) & 0 & 0\\\\\n",
        "       0 & 0 & 0 & 0 & 2(1+v) & 0 \\\\\n",
        "       0 & 0 & 0 & 0 & 0 & 2(1+v)\n",
        "\\end {bmatrix}\n",
        "\\begin {Bmatrix}\n",
        "       \\sigma_x \\\\\n",
        "       \\sigma_y \\\\\n",
        "       \\sigma_z \\\\\n",
        "       \\tau_{xy} \\\\\n",
        "       \\tau_{yz} \\\\\n",
        "       \\tau_{zx}\n",
        "\\end {Bmatrix}\n",
        "$$\n",
        "\n",
        "# Navier の方程式\n",
        "\n",
        "$$\n",
        "G \\left(\\nabla^2u_i + \\frac{1}{1-2\\nu}\\frac{\\partial \\epsilon_i}{\\partial x_i}\n",
        " + X_i \\right) = \\rho \\frac{\\partial^2 u_i}{\\partial t^2} \\quad (i=1,2,3)\n",
        "$$\n",
        "\n",
        "静的釣合い条件では、$i$ を $x,y,z$ で書くと、\n",
        "\n",
        "# 損失関数\n",
        "\n",
        "損失関数は、ニューラルネットワークの出力が連続体方程式を満たすようにする．具体的には、以下のような損失関数を定義：\n",
        "\n",
        "$$ L_{\\text{total}} = L_{\\text{PDE}} + L_{\\text{BC}} + L_{\\text{IC}}. $$\n",
        "\n",
        "- **PDE損失項** $L_{\\text{PDE}}$：ネットワークの出力がPDEを満たすようにする損失です。具体的には、次の手順で計算します：\n",
        "\n",
        "  1. ネットワークの出力 $u(x,t$) から、PDEに必要な偏微分を計算します（例えば、$\\frac{\\partial u}{\\partial t}$ や $\\frac{\\partial^2 u}{\\partial x^2}$）。\n",
        "  2. PDEの左辺 $\\mathcal{L}(u(x,t)$) を計算し、その結果がゼロに近くなるようにします。\n",
        "  3. 損失として、PDEの残差（$\\mathcal{L}(u(x,t))$）の二乗和を使用します。\n",
        "\n",
        "  例えば、熱方程式の場合のPDE損失は次のように書けます：\n",
        "\n",
        "  $$ L_{\\text{PDE}} = \\frac{1}{N_{\\text{PDE}}} \\sum_{i=1}^{N_{\\text{PDE}}} \\left(\\frac{\\partial u(x_i, t_i)}{\\partial t} - \\alpha \\frac{\\partial^2 u(x_i, t_i)}{\\partial x^2} \\right)^2, $$\n",
        "\n",
        "  ここで、$(x_i, t_i)$ はランダムにサンプリングされたポイントです。\n",
        "\n",
        "- **境界条件損失項** $L_{\\text{BC}}$：境界条件（例えば、空間の端点での値やフラックス条件）を満たすようにする損失です。\n",
        "\n",
        "  例えば、$x = 0$ と $x = L$ での境界条件が $u(0, t) = u_L$ と $u(L, t) = u_R$ である場合、次のように計算します：\n",
        "\n",
        "  $$ L_{\\text{BC}} = \\frac{1}{N_{\\text{BC}}} \\sum_{j=1}^{N_{\\text{BC}}} \\left(u(x_j, t_j) - u_{\\text{BC}}(x_j, t_j) \\right)^2, $$\n",
        "\n",
        "  ここで、$u_{\\text{BC}}$ は境界条件に従った値です。\n",
        "\n",
        "- **初期条件損失項** $L_{\\text{IC}}$：初期条件（例えば、$t=0$ のときの $u(x,0)$ の値）を満たすようにする損失です。初期条件損失は次のように書けます：\n",
        "\n",
        "  $$ L_{\\text{IC}} = \\frac{1}{N_{\\text{IC}}} \\sum_{k=1}^{N_{\\text{IC}}} \\left(u(x_k, 0) - u_{\\text{IC}}(x_k) \\right)^2, $$\n",
        "\n",
        "  ここで、$u_{\\text{IC}}$ は初期条件に従った値です。\n",
        "\n",
        "# 学習\n",
        "\n",
        "PINNsのトレーニングは、損失関数 $L_{\\text{total}}$ を最小化することによって行われます。通常、確率的勾配降下法（SGD）やそのバリエーション（例えばAdam）を使用します。トレーニングデータには、ランダムにサンプリングした空間座標と時間座標が含まれ、これらに対してネットワークの出力がPDE、境界条件、初期条件を満たすように調整されます。\n",
        "\n",
        "これが連続体方程式をPINNsの損失関数に組み込む基本的な流れです。具体的な実装には、数値微分、サンプリング戦略、ネットワークのアーキテクチャなどの詳細な調整が必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PYztHSxTGDq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YewL38xk-M8g"
      },
      "outputs": [],
      "source": [
        "num_points = 100\n",
        "\n",
        "n_in_out = 2\n",
        "n_make_out = 6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipGf4oUvZTKd"
      },
      "outputs": [],
      "source": [
        "# ニューラルネットワークの定義\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PINN, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_in_out, 32)\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        # self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, 128)\n",
        "        self.fc4 = nn.Linear(128, 256)\n",
        "        # self.bn4 = nn.BatchNorm1d(256)\n",
        "        self.fc5 = nn.Linear(256, 128)\n",
        "        self.fc6 = nn.Linear(128, 64)\n",
        "        self.fc7 = nn.Linear(64, n_make_out)\n",
        "        self.fc8 = nn.Sigmoid()\n",
        "        self.apply(self.init_weights) # 重み初期化を適用\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            init.xavier_uniform_(m.weight)  # Xavier初期化\n",
        "            if m.bias is not None:\n",
        "                init.zeros_(m.bias)  # バイアスはゼロ初期化\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
        "        # x = F.leaky_relu(self.bn2(self.fc2(x)), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc3(x), negative_slope=0.01)\n",
        "        # x = F.leaky_relu(self.bn4(self.fc4(x)), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc4(x), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc5(x), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc6(x), negative_slope=0.01)\n",
        "        x = F.leaky_relu(self.fc7(x), negative_slope=0.01)\n",
        "        x = self.fc8(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WkAC0GZcHV"
      },
      "outputs": [],
      "source": [
        "# Navier方程式の計算\n",
        "def derivatives(u, e, f, x):\n",
        "    u_x = torch.autograd.grad(outputs=u, inputs=x, grad_outputs=torch.ones_like(u),\n",
        "                              create_graph=True, retain_graph=True)[0]\n",
        "    u_xx = torch.autograd.grad(outputs=u_x[:, 0], inputs=x, grad_outputs=torch.ones_like(u_x[:, 0]),\n",
        "                               create_graph=True, retain_graph=True)[0][:, 0]\n",
        "    u_yy = torch.autograd.grad(outputs=u_x[:, 1], inputs=x, grad_outputs=torch.ones_like(u_x[:, 1]),\n",
        "                               create_graph=True, retain_graph=True)[0][:, 1]\n",
        "    u_e = torch.autograd.grad(outputs=e, inputs=x, grad_outputs=torch.ones_like(e),\n",
        "                              create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "    return u_xx + u_yy + 1/(1-0.3)*(u_e.sum(dim=1) + f.sum(dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfD47gY2Zrv-"
      },
      "outputs": [],
      "source": [
        "# 損失関数の定義\n",
        "def loss_function(model, x_bc, u_bc, x_ic, f_ic):\n",
        "\n",
        "    # 境界条件損失\n",
        "    y_bc = torch.rand((2 * N_bc, 1)) * L                # y座標はランダム\n",
        "    bc_pred = model(torch.cat((x_bc, y_bc), dim=1))     # MLPを通す\n",
        "    u_bc_pred = bc_pred[:, :2]                          # 変位を抽出\n",
        "    loss_bc = torch.mean((u_bc_pred - u_bc)**2)\n",
        "\n",
        "    # 初期条件損失\n",
        "    ic_pred = model(x_ic)                               # MLPを通す\n",
        "    f_ic_pred = ic_pred[:, 4:6]                         # 荷重を抽出\n",
        "    loss_ic = torch.mean((f_ic_pred - f_ic)**2)\n",
        "\n",
        "    return loss_bc + loss_ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdjLoHggsd9z"
      },
      "outputs": [],
      "source": [
        "# 訓練データの生成\n",
        "def generate_data(num_points):\n",
        "    # x と t の範囲を設定\n",
        "    x_plot = torch.linspace(-L, L, num_points).reshape(-1, 1) # [100, 1]\n",
        "    y_plot = torch.linspace(-L, L, num_points).reshape(-1, 1) # [100, 1]\n",
        "\n",
        "    # メッシュグリッドを作成\n",
        "    # X_g は x_plot の値を列方向に繰り返した行列\n",
        "    # Y_g は y_plot の値を行方向に繰り返した行列\n",
        "    # X_g=[100, 100],Y_g=[100, 100]\n",
        "    X_g, Y_g = torch.meshgrid(x_plot.squeeze(), y_plot.squeeze(), indexing='ij')\n",
        "\n",
        "    # Flatten 行優先で1行目のあとに2行目を並べる\n",
        "    # cat dim=1 で全組み合わせ (n,2)\n",
        "    X_flat = X_g.reshape(-1, 1) # [10000, 1]\n",
        "    Y_flat = Y_g.reshape(-1, 1) # [10000, 1]\n",
        "\n",
        "    # gen_data = torch.rand(num_points, 2) * 2 - 1  # [-1, 1]の範囲でサンプルを生成\n",
        "    gen_data = torch.cat((X_flat, Y_flat), dim=1)\n",
        "\n",
        "    return gen_data\n",
        "\n",
        "# ハイパーパラメータ\n",
        "num_epochs = 10000\n",
        "learning_rate = 0.001\n",
        "\n",
        "L = 1.0  # 空間範囲\n",
        "FX = 1.0  # 荷重\n",
        "N_bc = 100\n",
        "N_ic = 100\n",
        "\n",
        "# モデルの初期化\n",
        "model = PINN()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG1aO63IOkVd"
      },
      "outputs": [],
      "source": [
        "# ***** test *******************************************************************\n",
        "num_points=100                  # サンプル点数\n",
        "x = generate_data(num_points)   # サンプル点を生成\n",
        "x.requires_grad = True          # 勾配計算を可能にする\n",
        "u = model(x)                    # モデルの出力を計算\n",
        "\n",
        "print(x.shape)\n",
        "print('x=',x[:5])\n",
        "print(u.shape)\n",
        "print('u=',u[:5])\n",
        "\n",
        "re_ux = u[:, :2]                 # 配列を2次元に分離\n",
        "re_ex = u[:, 2:4]\n",
        "re_fx = u[:, 4:6]\n",
        "\n",
        "print('*** Output ***')\n",
        "print(re_ux.shape)\n",
        "print('re_ux=',re_ux[:5])\n",
        "print(re_ex.shape)\n",
        "print('re_ex=',re_ex[:5])\n",
        "print(re_fx.shape)\n",
        "print('re_fx=',re_fx[:5])\n",
        "\n",
        "# Navier方程式計算\n",
        "u_x = torch.autograd.grad(outputs=re_ux, inputs=x, grad_outputs=torch.ones_like(re_ux),\n",
        "                          create_graph=True, retain_graph=True)[0]\n",
        "# grad_outputs=torch.ones_like(u_x[:, 0])を指定するとxの最初の次元に関する勾配を計算\n",
        "u_xx = torch.autograd.grad(outputs=u_x[:, 0], inputs=x, grad_outputs=torch.ones_like(u_x[:, 0]),\n",
        "                            create_graph=True, retain_graph=True)[0][:, 0]\n",
        "u_yy = torch.autograd.grad(outputs=u_x[:, 1], inputs=x, grad_outputs=torch.ones_like(u_x[:, 1]),\n",
        "                            create_graph=True, retain_graph=True)[0][:, 1]\n",
        "u_e = torch.autograd.grad(outputs=re_ex, inputs=x, grad_outputs=torch.ones_like(re_ex),\n",
        "                          create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "print('*** Derivatives ***')\n",
        "print(u_x.shape)\n",
        "print('u_x=',u_x[:5])\n",
        "print(u_xx.shape)\n",
        "print('u_xx=',u_xx[:5])\n",
        "print(u_yy.shape)\n",
        "print('u_yy=',u_yy[:5])\n",
        "print(u_e.shape)\n",
        "print('u_e=',u_e[:5])\n",
        "print('u_e.sum=', u_e.sum(dim=1)[:5])\n",
        "print('u_xx + u_yy + 1/(1-2*0.3)*u_e.sum(dim=1) + re_fx.sum(dim=1)=', (u_xx + u_yy + 1/(1-2*0.3)*(u_e.sum(dim=1)) + (re_fx.sum(dim=1)))[:5])\n",
        "\n",
        "print('================================================')\n",
        "# 入力テンソル b を作成（勾配追跡を有効にする）\n",
        "b = torch.tensor(np.array([[-0.4506,  0.7401], [-0.1686, -0.2135]]), requires_grad=True)\n",
        "\n",
        "# 出力テンソル a を適当な計算から得る（ここでは b の2乗を例にする）\n",
        "a = b ** 2  # これは勾配を追跡する\n",
        "\n",
        "# 勾配計算\n",
        "grad = torch.autograd.grad(outputs=a, inputs=b, grad_outputs=torch.ones_like(a),\n",
        "                           create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "print('grad sample =', grad)  # 結果を表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ScxJM2p3Ft"
      },
      "outputs": [],
      "source": [
        "x_lwr = torch.full((3, 1), -1)\n",
        "x_upr = torch.full((3, 1), 1)\n",
        "y_rnd = torch.rand((2*3, 1)) * 1\n",
        "\n",
        "x_0 = torch.cat((x_lwr, x_upr), dim=0)\n",
        "ux_0 = torch.zeros((2 * 3, 1), dtype=torch.float32)\n",
        "print(x_0)\n",
        "print(torch.cat((x_0, y_rnd), dim=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xhfdrmGZ0n6"
      },
      "outputs": [],
      "source": [
        "# 訓練ループ\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # サンプル点を生成\n",
        "    x = generate_data(num_points)\n",
        "    x.requires_grad = True  # 勾配計算を可能にする\n",
        "\n",
        "    # モデルの出力を計算\n",
        "    u = model(x)\n",
        "\n",
        "    # デコーダからの出力を3次元の座標情報、変位情報と荷重情報に分ける\n",
        "    re_ux = u[:, :2]                 # 配列を2次元に分離\n",
        "    re_ex = u[:, 2:4]\n",
        "    re_fx = u[:, 4:6]\n",
        "\n",
        "    # Navierの残差を計算\n",
        "    derivatives_residual = derivatives(re_ux, re_ex, re_fx, x)\n",
        "\n",
        "    # 損失を計算\n",
        "    loss_nav = torch.mean(derivatives_residual**2)\n",
        "\n",
        "    # 境界条件と初期条件のサンプル\n",
        "    x_bc = torch.cat((torch.full((N_bc, 1), -L), torch.full((N_bc, 1), L)), dim=0) # x=-1,x=1の位置の配列\n",
        "    u_bc = torch.zeros((2 * N_bc, 1), dtype=torch.float32)                         # x=-1,x=1の境界条件の配列\n",
        "\n",
        "    x_ic = torch.tensor([[0, 0]], dtype=torch.float32)                             # 荷重点を原点に設定\n",
        "    f_ic = torch.tensor([[1, 1]], dtype=torch.float32)                             # 荷重をfx=1,fy=1で入れる\n",
        "\n",
        "    loss_cnd = loss_function(model, x_bc, u_bc, x_ic, f_ic)\n",
        "\n",
        "    loss = loss_nav + loss_cnd\n",
        "\n",
        "    # 逆伝播と最適化\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1-2HghpZ8S9"
      },
      "outputs": [],
      "source": [
        "# 結果の可視化\n",
        "def plot_solution(net, x, y):\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        # ネットワークの出力を計算\n",
        "        # 全組み合わせ[10000, 2]で、出力[10000, 6]を得る\n",
        "        u = net(torch.cat((x, y), dim=1)).numpy()\n",
        "        ux = np.sqrt(u[:, :1]**2 + u[:, 1:2]**2)\n",
        "\n",
        "        # x と t を 1D 配列に変換 ⇒ x= (10000,),t= (10000,)\n",
        "        x = x.numpy().flatten()\n",
        "        y = y.numpy().flatten()\n",
        "\n",
        "        # メッシュグリッドの生成\n",
        "        # 再度、[100, 100]にする、x,tをダブり抜きしてからmeshgrid\n",
        "        X, Y = np.meshgrid(np.unique(x), np.unique(y))\n",
        "\n",
        "        # u を 2D 配列に変換 [10000, 1]⇒[100, 100]\n",
        "        U = ux.reshape(len(np.unique(y)), len(np.unique(x)))\n",
        "\n",
        "        # プロット\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        contour = plt.contourf(X, Y, U, cmap='viridis')\n",
        "        plt.colorbar(contour)\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title('Predicted Solution')\n",
        "        plt.show()\n",
        "\n",
        "# x と t の範囲を設定\n",
        "x_plot = torch.linspace(-L, L, 100).reshape(-1, 1) # [100, 1]\n",
        "y_plot = torch.linspace(-L, L, 100).reshape(-1, 1) # [100, 1]\n",
        "\n",
        "# メッシュグリッドを作成\n",
        "# X_g は x_plot の値を列方向に繰り返した行列\n",
        "# Y_g は y_plot の値を行方向に繰り返した行列\n",
        "# X_g=[100, 100],Y_g=[100, 100]\n",
        "X_g, Y_g = torch.meshgrid(x_plot.squeeze(), y_plot.squeeze(), indexing='ij')\n",
        "\n",
        "# Flatten 行優先で1行目のあとに2行目を並べる\n",
        "# cat dim=1 で全組み合わせ (n,2)\n",
        "X_flat = X_g.reshape(-1, 1) # [10000, 1]\n",
        "Y_flat = Y_g.reshape(-1, 1) # [10000, 1]\n",
        "\n",
        "# 結果をプロット\n",
        "plot_solution(model, X_flat, Y_flat)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM5LpFTtgpi5lCTnCZZbDD+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}